{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools as it\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import mutual_info_regression as mir\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder as onehot\n",
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import xgboost as xgb\n",
    "from itertools import product\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.deterministic import Fourier\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../challenge_data/X_train.csv')\n",
    "Y_train = pd.read_csv('../challenge_data/Y_train.csv')\n",
    "X_test = pd.read_csv('../challenge_data/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Util.tools import *\n",
    "X_train_clean = fill_missing_with_average(X_train)\n",
    "X_test_clean = fill_missing_with_average(X_test)\n",
    "\n",
    "#drop those cols according to EDA\n",
    "X_train_clean=X_train_clean.drop([\"DE_FR_EXCHANGE\",\"DE_NET_IMPORT\",\"FR_NET_IMPORT\"],axis=1)\n",
    "X_test_clean=X_test_clean.drop([\"DE_FR_EXCHANGE\",\"DE_NET_IMPORT\",\"FR_NET_IMPORT\"],axis=1)\n",
    "\n",
    "#Split into DE & FR\n",
    "X_train_de = X_train_clean[X_train_clean['COUNTRY'] == 'DE']\n",
    "X_test_de = X_test_clean[X_test_clean['COUNTRY'] == 'DE']\n",
    "\n",
    "X_train_fr = X_train_clean[X_train_clean['COUNTRY'] == 'FR']\n",
    "X_test_fr = X_test_clean[X_test_clean['COUNTRY'] == 'FR']\n",
    "\n",
    "# merge TARGET\n",
    "X_train_de = pd.merge(X_train_de, Y_train, on='ID', how='inner').sort_values('DAY_ID')\n",
    "X_train_fr = pd.merge(X_train_fr, Y_train, on='ID', how='inner').sort_values('DAY_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select correlation bigger than 0.05 \n",
    "def get_sorted_correlations_and_features(X_train, threshold=0.05):\n",
    "    correlations = {}\n",
    "    for column in X_train.columns:\n",
    "        if column == 'TARGET':  \n",
    "            continue\n",
    "        corr, _ = spearmanr(X_train[column], X_train['TARGET'])\n",
    "        correlations[column] = corr\n",
    "\n",
    "    corr_df = pd.DataFrame(list(correlations.items()), columns=['Feature', 'Correlation'])\n",
    "    corr_df['Absolute_Correlation'] = corr_df['Correlation'].abs()\n",
    "    sorted_corr_df = corr_df.sort_values('Absolute_Correlation', ascending=False)\n",
    "    \n",
    "    selected_features = sorted_corr_df[sorted_corr_df['Absolute_Correlation'] >= threshold]['Feature']\n",
    "    features_selected = selected_features.tolist()\n",
    "    \n",
    "    return sorted_corr_df, features_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features for DE:\n",
      "['DE_RESIDUAL_LOAD', 'DE_NET_EXPORT', 'DE_WINDPOW', 'DE_GAS', 'DE_HYDRO', 'FR_WINDPOW', 'DE_COAL', 'DE_WIND', 'DE_LIGNITE', 'FR_DE_EXCHANGE', 'FR_WIND', 'FR_GAS', 'DE_CONSUMPTION', 'FR_RAIN', 'FR_HYDRO']\n",
      "Selected Features for FR:\n",
      "['CARBON_RET', 'GAS_RET', 'FR_WINDPOW', 'DE_HYDRO', 'DE_WINDPOW', 'DE_NET_EXPORT', 'FR_HYDRO', 'FR_COAL', 'DE_RAIN', 'COAL_RET', 'DE_RESIDUAL_LOAD', 'DE_CONSUMPTION']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
      "/Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
     ]
    }
   ],
   "source": [
    "sorted_corr_df_de, features_selected_de = get_sorted_correlations_and_features(X_train_de)\n",
    "print(\"Selected Features for DE:\")\n",
    "print(features_selected_de)\n",
    "\n",
    "sorted_corr_df_fr, features_selected_fr = get_sorted_correlations_and_features(X_train_fr)\n",
    "print(\"Selected Features for FR:\")\n",
    "print(features_selected_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "X_trainde, X_testde, Y_trainde, Y_testde = train_test_split(X_train_de[features_selected_de], X_train_de['TARGET'], test_size=0.2, random_state=42)\n",
    "X_trainfr, X_testfr, Y_trainfr, Y_testfr = train_test_split(X_train_fr[features_selected_fr], X_train_fr['TARGET'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def metric_train(output, truth):\n",
    "    return spearmanr(output, truth).correlation\n",
    "\n",
    "\n",
    "def get_model(model_name, best_param=None):\n",
    "    if model_name == 'dt':\n",
    "        model = DecisionTreeRegressor(**(best_param if best_param else {}))\n",
    "    elif model_name == 'bagging_ridge':\n",
    "        base_model = Ridge(**(best_param['base_model'] if best_param and 'base_model' in best_param else {}))\n",
    "        model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, **(best_param['model'] if best_param and 'model' in best_param else {}))\n",
    "    elif model_name == 'extra_trees':\n",
    "        model = ExtraTreesRegressor(**(best_param if best_param else {}))\n",
    "    elif model_name == 'rf':\n",
    "        model = RandomForestRegressor(**(best_param if best_param else {}))\n",
    "    elif model_name == 'bagging_knn':\n",
    "        base_model = KNeighborsRegressor(**(best_param['base_model'] if best_param and 'base_model' in best_param else {}))\n",
    "        model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, **(best_param['model'] if best_param and 'model' in best_param else {}))\n",
    "    elif model_name == 'bagging_svr':\n",
    "        base_model = SVR(**(best_param['base_model'] if best_param and 'base_model' in best_param else {}))\n",
    "        model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, **(best_param['model'] if best_param and 'model' in best_param else {}))\n",
    "    elif model_name == 'bagging_linear':\n",
    "        base_model = LinearRegression(**(best_param['base_model'] if best_param and 'base_model' in best_param else {}))\n",
    "        model = BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42, **(best_param['model'] if best_param and 'model' in best_param else {}))\n",
    "    elif model_name == 'adaboost':\n",
    "        model = AdaBoostRegressor(**(best_param if best_param else {}))\n",
    "    elif model_name == 'gradient_boosting':\n",
    "        model = GradientBoostingRegressor(**(best_param if best_param else {}))\n",
    "    else:\n",
    "        raise ValueError('Unknown Model')\n",
    "    return model\n",
    "        \n",
    "scorer_train = make_scorer(metric_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'dt',  # Decision Tree Regressor\n",
    "    'bagging_ridge',  # Bagging model based on Ridge regression\n",
    "    'extra_trees',  # Extra Trees Regressor\n",
    "    'rf',  # Random Forest Regressor\n",
    "    'bagging_knn',  # Bagging model based on KNN regression\n",
    "    'bagging_svr',  # Bagging model based on SVR\n",
    "    'bagging_linear',  # Bagging model based on Linear regression\n",
    "    'adaboost',  # AdaBoost Regressor\n",
    "    'gradient_boosting'  # Gradient Boosting Regressor\n",
    "]\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    model = get_model(model_name)\n",
    "    \n",
    "    # Train on DE dataset\n",
    "    model.fit(X_trainde, Y_trainde)\n",
    "    predictions_de = model.predict(X_testde)\n",
    "    score_de = metric_train(predictions_de, Y_testde)\n",
    "    \n",
    "    # Train on FR dataset\n",
    "    model.fit(X_trainfr, Y_trainfr)\n",
    "    predictions_fr = model.predict(X_testfr)  \n",
    "    score_fr = metric_train(predictions_fr, Y_testfr)\n",
    "    \n",
    "    # Overall Score\n",
    "    predictions_overall = np.concatenate((predictions_de, predictions_fr))\n",
    "    truth_overall = np.concatenate((Y_testde, Y_testfr))\n",
    "    score_overall = metric_train(predictions_overall, truth_overall)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'DE_Train_Score': score_de,\n",
    "        'FR_Train_Score': score_fr,\n",
    "        'Overall_Score': score_overall  \n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  DE_Train_Score  FR_Train_Score  Overall_Score\n",
      "0                 dt        0.166270        0.066704       0.106568\n",
      "1      bagging_ridge        0.491419        0.158929       0.313878\n",
      "2        extra_trees        0.189837        0.211448       0.201050\n",
      "3                 rf        0.338914        0.208304       0.262706\n",
      "4        bagging_knn        0.134548        0.102892       0.107535\n",
      "5        bagging_svr        0.412729        0.245307       0.320654\n",
      "6     bagging_linear        0.491704        0.156625       0.314120\n",
      "7           adaboost        0.364300        0.097936       0.201350\n",
      "8  gradient_boosting        0.296987        0.265450       0.271008\n"
     ]
    }
   ],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune only 1.bagging_ridge 2,extra_trees 3,random_forest 4,bagging_svr 5,bagging_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random_Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def rf_hyperparameter_optimization(X, y, cv=5, n_trials=10, seed=42):\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            \"n_estimators\": 100,  \n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 64),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 2, 64),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.2, 1.0),\n",
    "            \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 2, 64),\n",
    "            \"min_impurity_decrease\": trial.suggest_float(\"min_impurity_decrease\", 1e-2, 1.0, log=True),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "            \"ccp_alpha\": trial.suggest_float(\"ccp_alpha\", 1e-2, 1.0, log=True),\n",
    "        }\n",
    "\n",
    "        model = RandomForestRegressor(random_state=seed, **param)\n",
    "        \n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        return np.mean(rmse_scores)  \n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    return study.best_params\n",
    "\n",
    "rf_best_paramsde = rf_hyperparameter_optimization(X_trainde, Y_trainde, cv=5, n_trials=10, seed=42)\n",
    "rf_best_paramsfr = rf_hyperparameter_optimization(X_trainfr, Y_trainfr, cv=5, n_trials=10, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_best_paramsde {'max_depth': 12, 'min_samples_split': 7, 'min_samples_leaf': 36, 'max_features': 0.7677065026437544, 'max_leaf_nodes': 15, 'min_impurity_decrease': 0.02324164438742622, 'max_samples': 0.8636723851254833, 'ccp_alpha': 0.04849481120754413}\n",
      "rf_best_paramsfr {'max_depth': 11, 'min_samples_split': 46, 'min_samples_leaf': 19, 'max_features': 0.7589229311075041, 'max_leaf_nodes': 36, 'min_impurity_decrease': 0.3197690514647084, 'max_samples': 0.6611917747614966, 'ccp_alpha': 0.013952638388272324}\n"
     ]
    }
   ],
   "source": [
    "print(\"rf_best_paramsde\", rf_best_paramsde)\n",
    "print(\"rf_best_paramsfr\", rf_best_paramsfr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging_Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_ridge_hyperparameter_optimization(X, y, cv=5, n_trials=10, seed=42):\n",
    "    def objective(trial):\n",
    "\n",
    "        ridge_param = {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 0.1, 5.0, log=True),\n",
    "            \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        }\n",
    "        ridge_model = Ridge(**ridge_param)\n",
    "        \n",
    "\n",
    "        bagging_param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 5, 50),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.5, 1.0),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "            \"bootstrap_features\": trial.suggest_categorical(\"bootstrap_features\", [True, False]),\n",
    "        }\n",
    "        model = BaggingRegressor(base_estimator=ridge_model, random_state=seed, **bagging_param)\n",
    "        \n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        return np.mean(rmse_scores)  \n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    return study.best_params\n",
    "\n",
    "\n",
    "bagging_ridge_best_paramsde = bagging_ridge_hyperparameter_optimization(X_trainde, Y_trainde, cv=5, n_trials=10, seed=42)\n",
    "bagging_ridge_best_paramsfr = bagging_ridge_hyperparameter_optimization(X_trainfr, Y_trainfr, cv=5, n_trials=10, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging_ridge_best_paramsde {'alpha': 0.24530935689241493, 'fit_intercept': False, 'n_estimators': 32, 'max_samples': 0.7836691725672775, 'max_features': 0.7028963498535951, 'bootstrap': False, 'bootstrap_features': True}\n",
      "bagging_ridge_best_paramsfr {'alpha': 4.342477384102951, 'fit_intercept': False, 'n_estimators': 19, 'max_samples': 0.5047195405691205, 'max_features': 0.9636551412132504, 'bootstrap': True, 'bootstrap_features': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"bagging_ridge_best_paramsde\", bagging_ridge_best_paramsde)\n",
    "print(\"bagging_ridge_best_paramsfr\", bagging_ridge_best_paramsfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra_Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_trees_hyperparameter_optimization(X, y, cv=5, n_trials=10, seed=42):\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 5, 30),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 14),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 14),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"]),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "        }\n",
    "        \n",
    "        model = ExtraTreesRegressor(random_state=seed, **param)\n",
    "        \n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        return np.mean(rmse_scores)  \n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    return study.best_params\n",
    "\n",
    "extra_trees_best_paramsde = extra_trees_hyperparameter_optimization(X_trainde, Y_trainde, cv=5, n_trials=10, seed=42)\n",
    "extra_trees_best_paramsfr = extra_trees_hyperparameter_optimization(X_trainfr, Y_trainfr, cv=5, n_trials=10, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra_trees_best_paramsde {'n_estimators': 347, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': True}\n",
      "extra_trees_best_paramsfr {'n_estimators': 803, 'max_depth': 29, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"extra_trees_best_paramsde\", extra_trees_best_paramsde)\n",
    "print(\"extra_trees_best_paramsfr\", extra_trees_best_paramsfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bagging_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_svr_hyperparameter_optimization(X, y, cv=5, n_trials=10, seed=42):\n",
    "    def objective(trial):\n",
    "        svr_param = {\n",
    "            \"C\": trial.suggest_float(\"C\", 0.5, 10.0, log=True),\n",
    "            \"epsilon\": trial.suggest_float(\"epsilon\", 0.05, 1.0, log=True),\n",
    "            \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        }\n",
    "        svr_model = SVR(**svr_param)\n",
    "        \n",
    "        bagging_param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 5, 50),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.5, 1.0),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "            \"bootstrap_features\": trial.suggest_categorical(\"bootstrap_features\", [True, False]),\n",
    "        }\n",
    "        model = BaggingRegressor(base_estimator=svr_model, random_state=seed, **bagging_param)\n",
    "\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        return np.mean(rmse_scores) \n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    return study.best_params\n",
    "\n",
    "\n",
    "bagging_svr_best_paramsde = bagging_svr_hyperparameter_optimization(X_trainde, Y_trainde, cv=5, n_trials=10, seed=42)\n",
    "bagging_svr_best_paramsfr = bagging_svr_hyperparameter_optimization(X_trainfr, Y_trainfr, cv=5, n_trials=10, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging_svr_best_paramsde {'C': 2.073111784706136, 'epsilon': 0.3380017802887977, 'kernel': 'linear', 'n_estimators': 46, 'max_samples': 0.9738641495774639, 'max_features': 0.9776745869238563, 'bootstrap': False, 'bootstrap_features': True}\n",
      "bagging_svr_best_paramsfr {'C': 1.893184146184135, 'epsilon': 0.16610288223819752, 'kernel': 'linear', 'n_estimators': 29, 'max_samples': 0.7607259342411905, 'max_features': 0.6270018221375047, 'bootstrap': False, 'bootstrap_features': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"bagging_svr_best_paramsde\", bagging_svr_best_paramsde)\n",
    "print(\"bagging_svr_best_paramsfr\", bagging_svr_best_paramsfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bagging_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_linear_hyperparameter_optimization(X, y, cv=5, n_trials=10, seed=42):\n",
    "    def objective(trial):\n",
    "        bagging_param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 5, 100),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.5, 1.0),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "            \"bootstrap_features\": trial.suggest_categorical(\"bootstrap_features\", [True, False]),\n",
    "        }\n",
    "        base_model = LinearRegression()\n",
    "        model = BaggingRegressor(base_estimator=base_model, random_state=seed, **bagging_param)\n",
    "        \n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        return np.mean(rmse_scores)  \n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    return study.best_params\n",
    "\n",
    "bagging_linear_best_paramsde = bagging_linear_hyperparameter_optimization(X_trainde, Y_trainde, cv=5, n_trials=10, seed=42)\n",
    "bagging_linear_best_paramsfr = bagging_linear_hyperparameter_optimization(X_trainfr, Y_trainfr, cv=5, n_trials=10, seed=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging_linear_best_paramsde: {'n_estimators': 98, 'max_samples': 0.9884336579539678, 'max_features': 0.5972669840012728, 'bootstrap': False, 'bootstrap_features': True}\n",
      "bagging_linear_best_paramsfr: {'n_estimators': 76, 'max_samples': 0.7151840983643662, 'max_features': 0.6525844739910632, 'bootstrap': False, 'bootstrap_features': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"bagging_linear_best_paramsde:\", bagging_linear_best_paramsde)\n",
    "print(\"bagging_linear_best_paramsfr:\", bagging_linear_best_paramsfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(random_state=42)\n",
    "model.set_params(**rf_best_paramsde)  \n",
    "    \n",
    "model.fit(X_trainde, Y_trainde)\n",
    "predictions_de = model.predict(X_testde)  \n",
    "score_de = metric_train(predictions_fr, Y_testfr)\n",
    "score_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
      "/Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'rf', 'DE_Train_Score': 0.44697180573913853, 'FR_Train_Score': nan, 'Overall_Score': 0.22149684435088007}\n",
      "{'Model': 'bagging_ridge', 'DE_Train_Score': 0.4914188282647586, 'FR_Train_Score': 0.15892912856697147, 'Overall_Score': 0.31387770975233054}\n",
      "{'Model': 'extra_trees', 'DE_Train_Score': 0.3921511627906977, 'FR_Train_Score': 0.18879928960568318, 'Overall_Score': 0.26966255180613113}\n",
      "{'Model': 'bagging_svr', 'DE_Train_Score': 0.4127292039355993, 'FR_Train_Score': 0.24530683754529967, 'Overall_Score': 0.32065422949143874}\n",
      "{'Model': 'bagging_linear', 'DE_Train_Score': 0.4917039355992845, 'FR_Train_Score': 0.15662514699882402, 'Overall_Score': 0.31411993466594074}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "def apply_best_params(model, best_params):\n",
    "    # This function assumes best_params is a dictionary of parameters to be set\n",
    "    model.set_params(**best_params)\n",
    "    return model\n",
    "\n",
    "for model_name, best_params_de, best_params_fr in [\n",
    "    # Model names and best_params placeholders\n",
    "]:\n",
    "    # Initialize the model\n",
    "    model_de = get_model(model_name)\n",
    "    model_fr = get_model(model_name)\n",
    "    \n",
    "    # Apply best parameters - this step is conceptual\n",
    "    # You would need to ensure that best_params_* variables are dictionaries of parameters\n",
    "    model_de = apply_best_params(model_de, best_params_de)\n",
    "    model_fr = apply_best_params(model_fr, best_params_fr)\n",
    "\n",
    "\n",
    "for model_name, best_params_de, best_params_fr in [\n",
    "    ('rf', rf_best_paramsde, rf_best_paramsfr),\n",
    "    ('bagging_ridge', bagging_ridge_best_paramsde, bagging_ridge_best_paramsfr),\n",
    "    ('extra_trees', extra_trees_best_paramsde, extra_trees_best_paramsfr),\n",
    "    ('bagging_svr', bagging_svr_best_paramsde, bagging_svr_best_paramsfr),\n",
    "    ('bagging_linear', bagging_linear_best_paramsde, bagging_linear_best_paramsfr),\n",
    "]:\n",
    "\n",
    "    model_de = get_model(model_name, best_params_de)\n",
    "    model_de.fit(X_trainde, Y_trainde)\n",
    "    predictions_de = model_de.predict(X_testde)\n",
    "    score_de = metric_train(predictions_de, Y_testde)\n",
    "    \n",
    "    model_fr = get_model(model_name, best_params_fr)\n",
    "    model_fr.fit(X_trainfr, Y_trainfr)\n",
    "    predictions_fr = model_fr.predict(X_testfr)\n",
    "    score_fr = metric_train(predictions_fr, Y_testfr)\n",
    "\n",
    "    predictions_overall = np.concatenate((predictions_de, predictions_fr))\n",
    "    truth_overall = np.concatenate((Y_testde, Y_testfr))\n",
    "    score_overall = metric_train(predictions_overall, truth_overall)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'DE_Train_Score': score_de,\n",
    "        'FR_Train_Score': score_fr,\n",
    "        'Overall_Score': score_overall,\n",
    "    })\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
