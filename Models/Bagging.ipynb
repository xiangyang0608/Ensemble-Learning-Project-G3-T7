{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools as it\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import mutual_info_regression as mir\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder as onehot\n",
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "import xgboost as xgb\n",
    "from itertools import product\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.deterministic import Fourier\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../challenge_data/X_train.csv')\n",
    "Y_train = pd.read_csv('../challenge_data/Y_train.csv')\n",
    "X_test = pd.read_csv('../challenge_data/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Util.tools import *\n",
    "X_train_clean = fill_missing_with_average(X_train)\n",
    "X_test_clean = fill_missing_with_average(X_test)\n",
    "\n",
    "#drop those cols according to EDA\n",
    "X_train_clean=X_train_clean.drop([\"DE_FR_EXCHANGE\",\"DE_NET_IMPORT\",\"FR_NET_IMPORT\"],axis=1)\n",
    "X_test_clean=X_test_clean.drop([\"DE_FR_EXCHANGE\",\"DE_NET_IMPORT\",\"FR_NET_IMPORT\"],axis=1)\n",
    "\n",
    "#Split into DE & FR\n",
    "X_train_de = X_train_clean[X_train_clean['COUNTRY'] == 'DE']\n",
    "X_test_de = X_test_clean[X_test_clean['COUNTRY'] == 'DE']\n",
    "\n",
    "X_train_fr = X_train_clean[X_train_clean['COUNTRY'] == 'FR']\n",
    "X_test_fr = X_test_clean[X_test_clean['COUNTRY'] == 'FR']\n",
    "\n",
    "# merge TARGET\n",
    "X_train_de = pd.merge(X_train_de, Y_train, on='ID', how='inner').sort_values('DAY_ID')\n",
    "X_train_fr = pd.merge(X_train_fr, Y_train, on='ID', how='inner').sort_values('DAY_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select correlation bigger than 0.05 \n",
    "def get_sorted_correlations_and_features(X_train, threshold=0.05):\n",
    "    correlations = {}\n",
    "    for column in X_train.columns:\n",
    "        if column == 'TARGET':  \n",
    "            continue\n",
    "        corr, _ = spearmanr(X_train[column], X_train['TARGET'])\n",
    "        correlations[column] = corr\n",
    "\n",
    "    corr_df = pd.DataFrame(list(correlations.items()), columns=['Feature', 'Correlation'])\n",
    "    corr_df['Absolute_Correlation'] = corr_df['Correlation'].abs()\n",
    "    sorted_corr_df = corr_df.sort_values('Absolute_Correlation', ascending=False)\n",
    "    \n",
    "    selected_features = sorted_corr_df[sorted_corr_df['Absolute_Correlation'] >= threshold]['Feature']\n",
    "    features_selected = selected_features.tolist()\n",
    "    \n",
    "    return sorted_corr_df, features_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features for DE:\n",
      "['DE_RESIDUAL_LOAD', 'DE_NET_EXPORT', 'DE_WINDPOW', 'DE_GAS', 'DE_HYDRO', 'FR_WINDPOW', 'DE_COAL', 'DE_WIND', 'DE_LIGNITE', 'FR_DE_EXCHANGE', 'FR_WIND', 'FR_GAS', 'DE_CONSUMPTION', 'FR_RAIN', 'FR_HYDRO']\n",
      "Selected Features for FR:\n",
      "['CARBON_RET', 'GAS_RET', 'FR_WINDPOW', 'DE_HYDRO', 'DE_WINDPOW', 'DE_NET_EXPORT', 'FR_HYDRO', 'FR_COAL', 'DE_RAIN', 'COAL_RET', 'DE_RESIDUAL_LOAD', 'DE_CONSUMPTION']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
      "/Users/zhengwan/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4916: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n"
     ]
    }
   ],
   "source": [
    "sorted_corr_df_de, features_selected_de = get_sorted_correlations_and_features(X_train_de)\n",
    "print(\"Selected Features for DE:\")\n",
    "print(features_selected_de)\n",
    "\n",
    "sorted_corr_df_fr, features_selected_fr = get_sorted_correlations_and_features(X_train_fr)\n",
    "print(\"Selected Features for FR:\")\n",
    "print(features_selected_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "X_trainde, X_testde, Y_trainde, Y_testde = train_test_split(X_train_de[features_selected_de], X_train_de['TARGET'], test_size=0.2, random_state=42)\n",
    "X_trainfr, X_testfr, Y_trainfr, Y_testfr = train_test_split(X_train_fr[features_selected_fr], X_train_fr['TARGET'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def metric_train(output, truth):\n",
    "    return spearmanr(output, truth).correlation\n",
    "\n",
    "def get_model(model_name):\n",
    "    if model_name == 'dt':\n",
    "        return DecisionTreeRegressor()\n",
    "    elif model_name == 'bagging_ridge':\n",
    "        base_model = Ridge()  # Use Ridge regression as the base model\n",
    "        return BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42)\n",
    "    elif model_name == 'extra_trees':\n",
    "        return ExtraTreesRegressor()\n",
    "    elif model_name == 'rf':\n",
    "        return RandomForestRegressor()\n",
    "    elif model_name == 'bagging_knn':\n",
    "        base_model = KNeighborsRegressor()  # Use KNN regression as the base model\n",
    "        return BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42)\n",
    "    elif model_name == 'bagging_svr':\n",
    "        base_model = SVR()  # Use Support Vector Machine regression as the base model\n",
    "        return BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42)\n",
    "    elif model_name == 'bagging_linear':\n",
    "        base_model = LinearRegression()  # Use Linear regression as the base model\n",
    "        return BaggingRegressor(base_estimator=base_model, n_estimators=10, random_state=42)\n",
    "    elif model_name == 'adaboost':\n",
    "        return AdaBoostRegressor()\n",
    "    elif model_name == 'gradient_boosting':\n",
    "        return GradientBoostingRegressor()\n",
    "    else:\n",
    "        raise ValueError('Unknown Model')\n",
    "        \n",
    "scorer_train = make_scorer(metric_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'dt',  # Decision Tree Regressor\n",
    "    'bagging_ridge',  # Bagging model based on Ridge regression\n",
    "    'extra_trees',  # Extra Trees Regressor\n",
    "    'rf',  # Random Forest Regressor\n",
    "    'bagging_knn',  # Bagging model based on KNN regression\n",
    "    'bagging_svr',  # Bagging model based on SVR\n",
    "    'bagging_linear',  # Bagging model based on Linear regression\n",
    "    'adaboost',  # AdaBoost Regressor\n",
    "    'gradient_boosting'  # Gradient Boosting Regressor\n",
    "]\n",
    "\n",
    "# Train and evaluate models\n",
    "results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    model = get_model(model_name)\n",
    "    \n",
    "    # Train on DE dataset\n",
    "    model.fit(X_trainde, Y_trainde)\n",
    "    predictions_de = model.predict(X_testde)\n",
    "    score_de = metric_train(predictions_de, Y_testde)\n",
    "    \n",
    "    # Train on FR dataset\n",
    "    model.fit(X_trainfr, Y_trainfr)\n",
    "    predictions_fr = model.predict(X_testfr)  \n",
    "    score_fr = metric_train(predictions_fr, Y_testfr)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'DE_Train_Score': score_de,\n",
    "        'FR_Train_Score': score_fr\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model  DE_Train_Score  FR_Train_Score\n",
      "0                 dt        0.190454        0.116455\n",
      "1      bagging_ridge        0.491419        0.158929\n",
      "2        extra_trees        0.198083        0.157024\n",
      "3                 rf        0.265921        0.189728\n",
      "4        bagging_knn        0.134548        0.102892\n",
      "5        bagging_svr        0.412729        0.245307\n",
      "6     bagging_linear        0.491704        0.156625\n",
      "7           adaboost        0.286776        0.099414\n",
      "8  gradient_boosting        0.279215        0.278748\n"
     ]
    }
   ],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune only 1.bagging_ridge 2,extra_trees 3,random_forest 4,bagging_svr 5,bagging_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random_Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def rf_hyperparameter_optimization(X, y, cv=5, n_trials=10, seed=42):\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            \"n_estimators\": 100,  \n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 64),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 2, 64),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.2, 1.0),\n",
    "            \"max_leaf_nodes\": trial.suggest_int(\"max_leaf_nodes\", 2, 64),\n",
    "            \"min_impurity_decrease\": trial.suggest_float(\"min_impurity_decrease\", 1e-2, 1.0, log=True),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "            \"ccp_alpha\": trial.suggest_float(\"ccp_alpha\", 1e-2, 1.0, log=True),\n",
    "        }\n",
    "\n",
    "        model = RandomForestRegressor(random_state=seed, **param)\n",
    "        \n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        return np.mean(rmse_scores)  \n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    return study.best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-18 03:36:18,144] A new study created in memory with name: no-name-0ec983ed-529b-4b4b-96e3-5b2f433f086f\n",
      "[I 2024-02-18 03:36:19,173] Trial 0 finished with value: 1.0066674896558196 and parameters: {'max_depth': 8, 'min_samples_split': 15, 'min_samples_leaf': 43, 'max_features': 0.8185697975514876, 'max_leaf_nodes': 35, 'min_impurity_decrease': 0.19174870540761627, 'max_samples': 0.998922289931578, 'ccp_alpha': 0.07771970662659705}. Best is trial 0 with value: 1.0066674896558196.\n",
      "[I 2024-02-18 03:36:20,074] Trial 1 finished with value: 1.0063175953113985 and parameters: {'max_depth': 7, 'min_samples_split': 59, 'min_samples_leaf': 25, 'max_features': 0.2493305504944348, 'max_leaf_nodes': 59, 'min_impurity_decrease': 0.0961114127571237, 'max_samples': 0.5970929434316429, 'ccp_alpha': 0.5921555508316173}. Best is trial 1 with value: 1.0063175953113985.\n",
      "[I 2024-02-18 03:36:21,179] Trial 2 finished with value: 0.9802936640426765 and parameters: {'max_depth': 4, 'min_samples_split': 25, 'min_samples_leaf': 28, 'max_features': 0.3157989158094407, 'max_leaf_nodes': 20, 'min_impurity_decrease': 0.014231628952015142, 'max_samples': 0.8042184913920745, 'ccp_alpha': 0.014411968858996087}. Best is trial 2 with value: 0.9802936640426765.\n",
      "[I 2024-02-18 03:36:22,230] Trial 3 finished with value: 1.0066567517693943 and parameters: {'max_depth': 6, 'min_samples_split': 32, 'min_samples_leaf': 51, 'max_features': 0.8159908040786317, 'max_leaf_nodes': 60, 'min_impurity_decrease': 0.016728401379471118, 'max_samples': 0.6826844032535769, 'ccp_alpha': 0.5525346237798139}. Best is trial 2 with value: 0.9802936640426765.\n",
      "[I 2024-02-18 03:36:23,186] Trial 4 finished with value: 1.0001782519655142 and parameters: {'max_depth': 5, 'min_samples_split': 27, 'min_samples_leaf': 23, 'max_features': 0.5220091674606551, 'max_leaf_nodes': 64, 'min_impurity_decrease': 0.08886669236492244, 'max_samples': 0.5525844775438404, 'ccp_alpha': 0.12850033634844027}. Best is trial 2 with value: 0.9802936640426765.\n",
      "[I 2024-02-18 03:36:24,140] Trial 5 finished with value: 1.0067829504880375 and parameters: {'max_depth': 5, 'min_samples_split': 33, 'min_samples_leaf': 13, 'max_features': 0.6425214655689611, 'max_leaf_nodes': 29, 'min_impurity_decrease': 0.5571930581140102, 'max_samples': 0.8428858249823062, 'ccp_alpha': 0.013393642086867037}. Best is trial 2 with value: 0.9802936640426765.\n",
      "[I 2024-02-18 03:36:25,123] Trial 6 finished with value: 1.0067790594335597 and parameters: {'max_depth': 11, 'min_samples_split': 63, 'min_samples_leaf': 53, 'max_features': 0.7466769201123675, 'max_leaf_nodes': 13, 'min_impurity_decrease': 0.9621309394743162, 'max_samples': 0.7999365907147846, 'ccp_alpha': 0.9811570448215037}. Best is trial 2 with value: 0.9802936640426765.\n",
      "[I 2024-02-18 03:36:26,132] Trial 7 finished with value: 1.003820603061908 and parameters: {'max_depth': 4, 'min_samples_split': 14, 'min_samples_leaf': 7, 'max_features': 0.6661787609715303, 'max_leaf_nodes': 51, 'min_impurity_decrease': 0.1434286435129878, 'max_samples': 0.987161259109608, 'ccp_alpha': 0.052264076371657316}. Best is trial 2 with value: 0.9802936640426765.\n",
      "[I 2024-02-18 03:36:27,062] Trial 8 finished with value: 1.0065097274227592 and parameters: {'max_depth': 7, 'min_samples_split': 58, 'min_samples_leaf': 36, 'max_features': 0.5372351898585093, 'max_leaf_nodes': 29, 'min_impurity_decrease': 0.16600626054382003, 'max_samples': 0.740893106557114, 'ccp_alpha': 0.038839612944846544}. Best is trial 2 with value: 0.9802936640426765.\n",
      "[I 2024-02-18 03:36:27,977] Trial 9 finished with value: 1.0067188632829294 and parameters: {'max_depth': 6, 'min_samples_split': 62, 'min_samples_leaf': 51, 'max_features': 0.3840619341170342, 'max_leaf_nodes': 16, 'min_impurity_decrease': 0.02589569909203669, 'max_samples': 0.8521110873731594, 'ccp_alpha': 0.4915698271602454}. Best is trial 2 with value: 0.9802936640426765.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params {'max_depth': 4, 'min_samples_split': 25, 'min_samples_leaf': 28, 'max_features': 0.3157989158094407, 'max_leaf_nodes': 20, 'min_impurity_decrease': 0.014231628952015142, 'max_samples': 0.8042184913920745, 'ccp_alpha': 0.014411968858996087}\n"
     ]
    }
   ],
   "source": [
    "rf_best_params = rf_hyperparameter_optimization(X_trainde, Y_trainde, cv=5, n_trials=10, seed=42)\n",
    "print(\"best_params\", rf_best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2787481700146399"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestRegressor(random_state=42)\n",
    "model.set_params(**rf_best_params)  \n",
    "    \n",
    "# 训练模型\n",
    "model.fit(X_trainde, Y_trainde)\n",
    "predictions_de = model.predict(X_testde)  \n",
    "score_de = metric_train(predictions_fr, Y_testfr)\n",
    "score_de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging_Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_ridge_hyperparameter_optimization(X, y, cv=5, n_trials=10, seed=42):\n",
    "    def objective(trial):\n",
    "\n",
    "        ridge_param = {\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 0.1, 5.0, log=True),\n",
    "            \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
    "        }\n",
    "        ridge_model = Ridge(**ridge_param)\n",
    "        \n",
    "\n",
    "        bagging_param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 5, 50),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.5, 1.0),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "            \"bootstrap_features\": trial.suggest_categorical(\"bootstrap_features\", [True, False]),\n",
    "        }\n",
    "        model = BaggingRegressor(base_estimator=ridge_model, random_state=seed, **bagging_param)\n",
    "        \n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        return np.mean(rmse_scores)  \n",
    "\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    return study.best_params\n",
    "\n",
    "\n",
    "bagging_ridge_best_params = bagging_ridge_hyperparameter_optimization(X_trainde, Y_trainde, cv=5, n_trials=10, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳超参数： {'alpha': 1.0461108737828706, 'fit_intercept': False, 'n_estimators': 17, 'max_samples': 0.5542504452666337, 'max_features': 0.5992117149215204, 'bootstrap': True, 'bootstrap_features': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"bagging_ridge_best_params\", bagging_ridge_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra_Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_trees_hyperparameter_optimization(X, y, cv=5, n_trials=10, seed=42):\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 5, 30),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 14),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 14),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"]),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "        }\n",
    "        \n",
    "        model = ExtraTreesRegressor(random_state=seed, **param)\n",
    "        \n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        return np.mean(rmse_scores)  \n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    return study.best_params\n",
    "\n",
    "extra_trees_best_params = extra_trees_hyperparameter_optimization(X_trainde, Y_trainde, cv=5, n_trials=10, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"extra_trees_best_params\", extra_trees_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bagging_svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_svr_hyperparameter_optimization(X, y, cv=5, n_trials=10, seed=42):\n",
    "    def objective(trial):\n",
    "        svr_param = {\n",
    "            \"C\": trial.suggest_float(\"C\", 0.5, 10.0, log=True),\n",
    "            \"epsilon\": trial.suggest_float(\"epsilon\", 0.05, 1.0, log=True),\n",
    "            \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        }\n",
    "        svr_model = SVR(**svr_param)\n",
    "        \n",
    "        bagging_param = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 5, 50),\n",
    "            \"max_samples\": trial.suggest_float(\"max_samples\", 0.5, 1.0),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.5, 1.0),\n",
    "            \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "            \"bootstrap_features\": trial.suggest_categorical(\"bootstrap_features\", [True, False]),\n",
    "        }\n",
    "        model = BaggingRegressor(base_estimator=svr_model, random_state=seed, **bagging_param)\n",
    "\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "        rmse_scores = np.sqrt(-scores)\n",
    "        return np.mean(rmse_scores) \n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    return study.best_params\n",
    "\n",
    "\n",
    "bagging_svr_best_params = bagging_svr_hyperparameter_optimization(X_trainde, Y_trainde, cv=5, n_trials=10, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging_svr_best_params {'C': 0.9100753035718627, 'epsilon': 0.7212504348863487, 'kernel': 'linear', 'n_estimators': 31, 'max_samples': 0.7818924468563151, 'max_features': 0.7231084111243953, 'bootstrap': False, 'bootstrap_features': True}\n"
     ]
    }
   ],
   "source": [
    "print(\"bagging_svr_best_params\", bagging_svr_best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd75362e27048f1ead3b65beb4812b1da3d387150557ce53b099093c32022a5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
